## 前置准备：

极大似然的概念

矩阵求导的基本法则

之前内容讲如何使用概率模型推导损失函数，并解释了如何对损失函数进行求导。

在这一讲中，讲如何将之前的内容转化为真实的生产力，及推导之前说过的模型。

在这一章当中，将会串联第二章到第四章的所有核心知识点

这一章的难度，大概相当于阿姆斯特丹大学的一个随堂作业。

这门课程 4 周 12 节课，共考试 13 次，8 次理论，4 次实操，最后一次大考；一次不过全盘否决 ，实操课内容为给定一篇 paper，从早上九点到晚上 12 点之前，必须实现并作出 Monte Carlo 模拟结果，并写成报告。

难点：

从 paper 到实现；

如何在几核的电脑上跑几百万次模拟，在本章最后，简化版的习题。



## 从模型到代码的过程

非常精细的写出模型当中的每一步

检查是否有标记的错误

使用推导当中的写法，忽略 pep8 进行开发

使用最笨的方式进行开发，不要考虑效率

使用 Monte Carlo 检查简单的模型是否正常



## 逻辑回归基本设定

$$
定义 σ : x → \frac{1}{1+exp(−x)}\\
逻辑回归的概率密度函数为 p_β(xi) = σ(x^t_iβ), 其中β为未知参数，x_i为
解释变量\\
负的对数似然函数为：\\
−\sum _iy_ilog(p_β(x_i)) + (1 − y_i)log(1 − p_β(xi))\\
现在需要做的是求其导数
$$

由于矩阵形式非常简单，所以最麻烦的部分其实是对一堆非线性的函数的推导，我们当然可以手推，但是问题在于，手推很容易出错 。

所以这时候要用到sympy ,可以计算出对数似然函数为：
$$
-\sum_i(y_iexp(-x^t_iβ)/(1+exp(-x^t_iβ))-(1-y_i)exp(x^t_iβ)/(1+exp(x^t_iβ))x_i
$$
括号里面的东西还是有点复杂，不妨再试试看 sympy 是否能帮我们化简:
$$
-\sum_i(y_i-σ(x_i^tβ))x_i
$$


## 使用 jax 实现自动求导过程并测试整体的正确性



### 关于 Jax 实践要点

尽可能用接近于 Numpy 的形式进行了实现，通过 Jax 的 Autograd 机制判断了是否求导准确，这一点是非常重要的。

如果没有 Jax，可能只能用 PyTorch 或者 TF 计算 Autograd（会相对来说更麻烦一些）。

如果这些 Autograd 的机制都没有，这时候我们面临的问题就更为麻烦，通常使用 Finite Difference 进行调试。

【注意】这种调试往往会造成很大的误差，所以有时候很难进行判断。



### 继续实践之前..

得到了导数，接下来该使用一些优化方法来一步步进行优化了。不幸的是，就目前对于Python 来说，不论是 jax.scipy.opimitize.minimize 还是 scipy.minimize 都有巨大的问题。

就 Jax 来说，大部分优化算法都没有实现，实现的部分也有 bug。

 就 scipy 来说，问题更大一些。



### Scipy 的问题

Scipy 优化的最大的问题在于，scipy 包装的是 fortran 77 的优化路径。

在 fortran 77 的优化路径当中，其整体只使用了双精度，并对于各种存在的精度问题没有做任何优化。

这使得在实际使用中，scipy 几乎永远不会得到正确的结论，因为各种 numerical issue 都会出现，并且不能修复。

这也是为什么在科学计算中，大部分人还是使用 matlab 的原因。



## 可识别性问题

$$
假设我们的目标是y，我们有 x_1, x_2, x_3 三个变量，并且 x_3 = 2x_1 + x2 ，\\
是否能找到 β_1, β_2, β_3 使得\sum _i (y_i − β_1x_{i1} − β_{2}x_{i2} − β_3x_{i3}) ^2 最小。\\
如果可能，我们能找到多少个？\\
答：无数个
$$



以上问题称之为不可识别性的问题：

简单来说，**对于一个模型来说，存在（潜在）无穷多个解使得该模型对应的损失函数最小。**

对于存在线性表达式的模型来说，这种情况是极其麻烦的。这里面一种很常见的情况，称之为多重共线性，指的是一些变量可以用其他变量的线性组合表达出来。

对于 R 来说，这些情况一般可以自动处理，即找到**极大线性无关组**。

极大线性无关组：每个向量不能用另外的代替。

很不幸的是，在 python 中，scipy 的实现极烂（大约比正常 C++ 实现慢 100 万倍）



Q：One-hot 编码输入逻辑回归之后是否可以正常求解？

One-hot：

One-hot 和常数项之间的关系：

如果有常数项的话，那么 one-hot 是不可以加入的，原因在于 one-hot 编码加起来等于 1。

必须考虑常数项来推断是否可以，如果没有任何常数项的话（以及其他输入是可以的）

为什么要加常数项：假设我们用“受教育年限”对“工资”做回归，如 果我们我们不加常数项，则等于我们认为未受过的教育的人的工资应该是 0，这显然是不符合实际的。



## 关于 Tobit 模型的推导



## Proximal Methods 近似方法的实现

目标是实现对数似然函数加上 l_1 损失的情况。

假设对数似然函数为 l_β(X, y), 其中 β 为待估参数，而 X, y 为数据，目标是最小化:
$$
−l_β(X, y) + λ||β||_1，其中 λ ≥ 0 为惩罚参数
$$
如何提升模型的效果?

之前的学习中，使用的 step size 都是固定的；

在这种情况，结果类似于梯度下降； 

是否有办法采用不同的 step size 呢？



## 关于 Proximal Methods 的一些应用的说明

Proximal methods 主要应用在 l1 正则化的函数估计上；

这类方法还有很多，例如Efron et al. (2004) 和Garrigues and Ghaoui (2008) 等。

目前在深度学习上也开始出现应用 (Yun, Lozano, and Yang 2020)

 **Proximal Methods见文档**



## 如何对自己的算法 debug 

首先务必检查数学推导是否正确：一般来说，最好的方法是和其他材料 做交叉验证

其次务必检查每一步是否都有合适的结果

最后运行整个算法的时候，需要注意： 

算法是否真的收敛了？ 

是否有 overflow 和 underflow? 

在多大情况下，算法会运行到一个局部最优？ 

是否可以通过调整初始值的方法加速收敛？ 

是否可以改变 line_search 的方向？

